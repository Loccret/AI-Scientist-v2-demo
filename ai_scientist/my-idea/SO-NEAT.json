[
    {
        "Name": "so_neat",
        "Title": "SO-NEAT: Self-Organizing NeuroEvolution of Augmenting Topologies",
        "Short Hypothesis": "Integrating within-lifetime synaptic plasticity and structural self-organization mechanisms into NEAT will enable faster convergence, better generalization, and enhanced adaptability to task changes without catastrophic interference, as these elements allow networks to adapt dynamically during evolution, which is not achievable through random mutations alone.",
        "Related Work": "NEAT is a well-established neuroevolution algorithm that evolves neural network topologies through speciation and historical markings. While some extensions like HyperNEAT incorporate indirect encoding or plasticity, none fully integrate self-organization principles such as homeostatic activity regulation and utility-driven structural changes within the NEAT framework. This proposal is novel because it embeds these biological inspirations directly into the evolutionary process, moving beyond simple parameter tuning or additive plasticity, and is not a trivial extension of existing work.",
        "Abstract": "SO-NEAT augments the NeuroEvolution of Augmenting Topologies (NEAT) algorithm with self-organization and plasticity to improve efficiency and robustness. It incorporates within-lifetime synaptic plasticity using local rules and neuromodulators, structural self-organization via homeostatic activity targets and utility-based node/synapse birth-death, and multi-objective selection balancing task fitness with parsimony and stability. We hypothesize that SO-NEAT will reduce the number of evaluations needed for convergence, enhance generalization under distribution shift, and enable online adaptation. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will validate these claims against standard NEAT.",
        "Experiments": [
            "XOR Problem: Compare SO-NEAT and NEAT on the number of evaluations required to achieve 100% accuracy. Implement plasticity as Hebbian learning with decay, and structural changes based on activity utility thresholds.",
            "Non-linear Classification (Circles, Moons, Spiral): Train both algorithms and test on held-out data to measure generalization error (mean squared error or accuracy). Use a multi-objective fitness function that includes task performance and network sparsity.",
            "Online Adaptation: Introduce a distribution shift mid-evolution (e.g., change data distribution for Circles task) and measure performance drop and recovery time. Metrics: fitness, evaluations, generalization error, and network complexity (number of nodes and connections)."
        ],
        "Risk Factors and Limitations": [
            "Increased computational overhead per evaluation due to plasticity and self-organization updates, which could slow down evolution.",
            "Sensitivity to hyperparameters for plasticity and self-organization mechanisms, requiring careful tuning.",
            "Potential for overfitting if self-organization priors are too strong, reducing exploration.",
            "Implementation complexity might introduce bugs or inefficiencies compared to standard NEAT."
        ]
    },
    {
        "Name": "gnn_predictive_neat",
        "Title": "GNN-Predictive NEAT: Accelerating NeuroEvolution with Graph Neural Network-based Fitness Prediction",
        "Short Hypothesis": "A Graph Neural Network (GNN) can be trained to predict the fitness of neural network topologies from their graph structure, enabling pre-screening of mutations in NEAT to reduce the number of expensive fitness evaluations, which will accelerate evolution without sacrificing solution quality. This approach is novel because it leverages GNNs\u2014which excel at capturing structural patterns\u2014for fitness prediction in neuroevolution, a context where traditional surrogate models are rarely used and not tailored for topological graphs.",
        "Related Work": "NEAT and its variants rely on direct fitness evaluations, which are computationally costly. Surrogate-assisted evolutionary algorithms often use simple models like Gaussian processes for parameter optimization, but they do not handle graph-structured topologies effectively. GNNs have been applied to various prediction tasks involving graphs (e.g., material science, fraud detection), but not to predict fitness in neuroevolution. This proposal distinguishes itself by specifically designing a GNN to model neural network topologies as graphs and predict their fitness, integrating it into NEAT's mutation and selection process\u2014a non-trivial extension that addresses the unique challenges of topological evolution.",
        "Abstract": "We introduce GNN-Predictive NEAT, a novel extension of the NeuroEvolution of Augmenting Topologies algorithm that uses a Graph Neural Network (GNN) to predict the fitness of neural networks based on their topology. This allows for efficient pre-screening of mutations: only candidates with high predicted fitness undergo full evaluation, reducing computational cost. We hypothesize that this method will significantly decrease the number of evaluations needed for convergence while maintaining competitive solution quality. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will compare GNN-Predictive NEAT against standard NEAT, measuring evaluations to convergence, final fitness, and generalization error. This approach leverages GNNs' ability to capture structural dependencies, offering a new direction for accelerating neuroevolution.",
        "Experiments": [
            "Benchmark Tasks: Use XOR, Circles, Moons, and Spiral classification tasks. Implement GNN-Predictive NEAT and standard NEAT for comparison.",
            "GNN Training: Collect a dataset of neural network topologies (represented as graphs with nodes for neurons and edges for connections) and their fitness values from initial NEAT runs. Train a GNN regression model to predict fitness from topology features (e.g., number of nodes, connectivity patterns).",
            "Integration into NEAT: During evolution, for each new mutation, use the trained GNN to predict fitness. Only evaluate mutations with predicted fitness above a threshold (e.g., top 50%) via actual simulation; others are discarded or stored for potential later use.",
            "Metrics: Record the number of fitness evaluations required to reach convergence (e.g., 95% accuracy), final task performance (accuracy or MSE), generalization error on test data, and computational time. Compare against baseline NEAT.",
            "Ablation Study: Variate the GNN architecture (e.g., GCN, GAT) and prediction threshold to analyze sensitivity and optimal settings."
        ],
        "Risk Factors and Limitations": [
            "Initial GNN training requires a dataset from NEAT runs, which might be biased or small, leading to poor prediction accuracy early on.",
            "GNN predictions could be inaccurate, causing beneficial mutations to be discarded or poor ones to be promoted, potentially hindering evolution.",
            "Increased computational overhead from GNN inference per mutation, though it may be offset by reduced evaluations.",
            "The approach may not generalize well to complex tasks where topology-fitness relationships are highly non-linear or noisy.",
            "Hyperparameter tuning for the GNN and prediction threshold adds complexity compared to standard NEAT."
        ]
    },
    {
        "Name": "modular_speciation_neat",
        "Title": "Modular-Speciation NEAT: Diversifying Evolution through Modularity-Based Speciation",
        "Short Hypothesis": "Incorporating modularity metrics into NEAT's speciation mechanism will enhance population diversity and exploration by grouping networks based on modularity similarity, leading to improved performance and generalization, as this provides a biologically-inspired axis for differentiation that complements topological distance and encourages the evolution of reusable, robust modules.",
        "Related Work": "NEAT uses compatibility distance for speciation based on topological differences. While extensions like HyperNEAT focus on indirect encoding, and recent work by Munn and Gallagher (2022) explored modularity in NEAT using MAP-Elites with a modularity objective, no prior work has integrated modularity directly into NEAT's core speciation process. This proposal distinguishes itself by modifying speciation to include modularity as a key factor, which is not a trivial extension and could lead to novel evolutionary dynamics by promoting diversity along modularity dimensions, unlike additive fitness objectives or external quality-diversity algorithms.",
        "Abstract": "We propose Modular-Speciation NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that incorporates modularity into the speciation process. By defining species based on both topological compatibility and modularity similarity (using graph-based metrics like Q-modularity), we aim to enhance population diversity and exploration. We hypothesize that this approach will yield neural networks with better performance, generalization, and adaptability due to the emergence of modular architectures. Experiments on benchmarks such as XOR, Circles, Moons, and Spiral will compare Modular-Speciation NEAT against standard NEAT, measuring fitness, generalization error, modularity scores, and species count. This work introduces a novel way to leverage modularity in neuroevolution, moving beyond fitness-based optimization to structural diversity management.",
        "Experiments": [
            "Benchmark Tasks: Implement Modular-Speciation NEAT and standard NEAT on XOR, Circles, Moons, and Spiral classification tasks. For Modular-Speciation NEAT, modify the compatibility distance calculation to include a modularity term: distance = topological_distance + \u03b1 * |modularity_diff|, where \u03b1 is a weight hyperparameter, and modularity is computed using the Q-modularity index from graph theory applied to the network topology.",
            "Performance Comparison: Record the number of evaluations to convergence (e.g., 95% accuracy), final task accuracy or MSE, and generalization error on held-out test data for both algorithms.",
            "Modularity Analysis: Compute and compare the average modularity score (Q-modularity) of evolved networks across generations for both algorithms, using networkx or similar libraries.",
            "Species Dynamics: Track the number and size of species over evolution to assess diversity. Compare speciation patterns between standard and Modular-Speciation NEAT.",
            "Ablation Study: Variate the weight \u03b1 in the compatibility distance to analyze its impact on performance and modularity, and find an optimal balance."
        ],
        "Risk Factors and Limitations": [
            "Increased computational overhead per generation due to modularity computation for each network, which could slow down evolution, especially for large topologies.",
            "Sensitivity to the definition and calculation of modularity; different modularity metrics might yield varying results, requiring careful selection and validation.",
            "Potential for over-segmentation of species if modularity differences are too emphasized, leading to inefficient evolution or slower convergence.",
            "Hyperparameter tuning for \u03b1 and modularity computation adds complexity compared to standard NEAT.",
            "May not generalize well to tasks where modularity is not beneficial or is difficult to define, limiting applicability."
        ]
    },
    {
        "Name": "curriculum_neat",
        "Title": "Curriculum-NEAT: Accelerating NeuroEvolution with Task-Based Curriculum Learning",
        "Short Hypothesis": "Integrating a curriculum learning framework into NEAT, where neural networks are evolved on a sequence of tasks from easy to hard, will reduce the number of evaluations needed for convergence and enhance generalization and transfer learning by providing a structured learning path that mimics human-like progressive skill acquisition, which is not achievable through random task presentation or single-task evolution in standard NEAT.",
        "Related Work": "NEAT and its variants focus on evolving networks for a single task without structured task ordering. While curriculum learning has been explored in reinforcement learning (e.g., in meta-learning setups like MAML) and some evolutionary algorithms (e.g., in quality-diversity approaches), it has not been systematically integrated into NEAT. Recent work by Nisioti et al. (GECCO 2025) compares neuroevolution and RL in transfer learning with curriculum benchmarks, but it does not modify NEAT's core algorithm. This proposal is novel because it embeds curriculum learning directly into NEAT's evolutionary process, using task difficulty to guide mutation and selection\u2014a non-trivial extension that leverages curriculum principles for efficiency gains in neuroevolution.",
        "Abstract": "We propose Curriculum-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that incorporates curriculum learning to accelerate evolution and improve generalization. By presenting tasks in an easy-to-hard sequence, Curriculum-NEAT guides the population through progressively challenging environments, reducing wasted evaluations on overly difficult mutations early on. We hypothesize that this approach will decrease the number of fitness evaluations required for convergence, enhance performance on target tasks, and facilitate better transfer to novel tasks. Experiments on benchmarks including XOR, Circles, Moons, and Spiral\u2014with curricula designed from simple to complex variants\u2014will validate these claims against standard NEAT and ablated versions. This work introduces a structured learning paradigm to neuroevolution, offering a new direction for efficient and adaptable AI systems.",
        "Experiments": [
            "Curriculum Design: Create task curricula for each benchmark (e.g., for XOR, start with 2-input XOR, then 3-input, and finally standard XOR; for Circles, use smaller radii or fewer points before full complexity). Implement Curriculum-NEAT to evolve networks on this sequence, transitioning to harder tasks based on performance thresholds (e.g., move to next task when fitness > 90% for current task).",
            "Comparison with Baselines: Run standard NEAT (evolving only on the final task) and Curriculum-NEAT on the same benchmarks. Metrics: number of fitness evaluations to convergence (e.g., 95% accuracy on final task), final accuracy or MSE, and generalization error on held-out test data.",
            "Transfer Learning Evaluation: After evolution, test networks on a related but unseen task (e.g., evolve on Circles curriculum, test on Ellipses dataset) to measure transfer performance. Metrics: accuracy or MSE on transfer task.",
            "Ablation Study: Variate the curriculum order (easy-to-hard vs. random vs. hard-to-easy) to isolate the effect of task sequencing. Also, test sensitivity to transition thresholds.",
            "Complexity Analysis: Record network size (nodes, connections) over generations to ensure curriculum does not lead to premature overfitting or underfitting."
        ],
        "Risk Factors and Limitations": [
            "Designing effective curricula may require domain knowledge or heuristic choices, which could be suboptimal and limit generalizability.",
            "Increased computational overhead from managing multiple tasks and transitions, though it may be offset by faster convergence.",
            "Risk of negative transfer if curriculum tasks are not well-aligned with the target task, potentially harming performance.",
            "Hyperparameter sensitivity (e.g., transition thresholds) adds tuning complexity compared to standard NEAT.",
            "May not outperform in tasks where difficulty progression is unclear or non-monotonic."
        ]
    },
    {
        "Name": "robust_so_neat",
        "Title": "Robust-SO-NEAT: Enhancing Adversarial Robustness through Self-Organization in NeuroEvolution",
        "Short Hypothesis": "Integrating self-organization mechanisms (synaptic plasticity and homeostatic regulation) into NEAT will evolve neural networks that are more robust to adversarial attacks, as these mechanisms foster intrinsic stability and adaptability during evolution, reducing vulnerability to perturbations without requiring explicit adversarial training. This is the best setting because self-organization provides a natural, continuous adaptation that complements evolution, and simpler methods like adding noise or adversarial training during evolution do not leverage structural and synaptic adaptability in the same way.",
        "Related Work": "NEAT is a foundational neuroevolution algorithm, and extensions like SO-NEAT introduce self-organization for efficiency. Work on adversarial robustness in neuroevolution (e.g., Operiano et al., 2021) uses gradient misalignment and architecture search, but it does not incorporate self-organization or plasticity within the NEAT framework. This proposal is novel because it embeds self-organization directly into NEAT's evolutionary process to enhance robustness, moving beyond post-hoc methods or separate robustness objectives, and is not a trivial extension of existing work.",
        "Abstract": "We propose Robust-SO-NEAT, an extension of NeuroEvolution of Augmenting Topologies that integrates self-organization principles\u2014specifically, within-lifetime synaptic plasticity and homeostatic activity regulation\u2014to improve adversarial robustness. By evolving networks that adapt dynamically to perturbations through plasticity and maintain stability via homeostasis, we hypothesize that Robust-SO-NEAT will yield networks with higher accuracy under adversarial attack compared to standard NEAT and robustness-focused variants. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will involve generating simple adversarial examples (e.g., FGSM attacks) and measuring robustness metrics. This approach offers a new pathway to robust AI by combining evolutionary and self-organizing dynamics.",
        "Experiments": [
            "Benchmark Tasks with Adversarial Attacks: Implement Robust-SO-NEAT and compare against standard NEAT and a baseline with simple adversarial training (e.g., evolving on adversarial examples). Use tasks like XOR, Circles, Moons, and Spiral. For each, generate adversarial examples using Fast Gradient Sign Method (FGSM) with a small epsilon (e.g., 0.1).",
            "Self-Organization Mechanisms: Incorporate Hebbian plasticity with decay for synaptic changes and homeostatic regulation to maintain neuronal activity within bounds. For example, adjust weights based on local activity and prune/add connections based on utility metrics.",
            "Robustness Evaluation: After evolution, test networks on clean and adversarial test sets. Metrics: accuracy on clean data, accuracy under adversarial attack (robust accuracy), and generalization gap (difference between clean and adversarial accuracy).",
            "Evolutionary Performance: Record number of evaluations to convergence on clean tasks, and monitor fitness during evolution to ensure self-organization does not hinder task performance.",
            "Ablation Study: Variate the strength of plasticity and homeostasis parameters to analyze their impact on robustness and task performance."
        ],
        "Risk Factors and Limitations": [
            "Increased computational cost due to self-organization updates and adversarial example generation during testing.",
            "Sensitivity to hyperparameters for plasticity and homeostasis, requiring careful tuning.",
            "Potential trade-off between task performance and robustness if self-organization priors are too strong.",
            "May not scale well to high-dimensional or complex tasks beyond simple benchmarks.",
            "Adversarial attacks used are simplistic (e.g., FGSM), which might not capture full robustness challenges."
        ]
    },
    {
        "Name": "sparse_neat",
        "Title": "Sparse-NEAT: Explicit Sparsity Optimization in NeuroEvolution of Augmenting Topologies",
        "Short Hypothesis": "Explicitly incorporating a sparsity objective into NEAT's fitness function will evolve neural networks that are both high-performing and computationally efficient due to reduced connectivity, without sacrificing generalization, as sparsity acts as a built-in regularizer during evolution. This is the best setting because it directly influences the evolutionary process towards sparse topologies, unlike post-hoc pruning methods or implicit sparsity in standard NEAT, and it addresses efficiency concerns that are critical for real-world applications.",
        "Related Work": "NEAT evolves neural network topologies without explicit sparsity optimization, though it can produce sparse networks implicitly. Work on sparse neural networks often involves post-training pruning or regularization (e.g., L1 regularization), but these are not integrated into the evolutionary process. Proposals like SO-NEAT include parsimony in multi-objective selection, but they focus on broader self-organization mechanisms. This proposal is novel because it makes sparsity a primary and explicit objective in NEAT's fitness function, which is not a trivial extension and could lead to more efficient evolution and deployment-ready networks.",
        "Abstract": "We propose Sparse-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that explicitly optimizes for sparsity by incorporating a sparsity term into the fitness function. The fitness is defined as task performance minus a weighted penalty based on the number of connections, encouraging the evolution of networks with fewer parameters while maintaining accuracy. We hypothesize that Sparse-NEAT will yield networks with improved computational efficiency, better generalization due to regularization, and faster convergence in some cases. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will compare Sparse-NEAT against standard NEAT, measuring accuracy, sparsity (number of connections), generalization error, and evaluation count.",
        "Experiments": [
            "Implement Sparse-NEAT with fitness function: fitness = accuracy (or 1 - MSE) - \u03bb * number_of_connections, where \u03bb is a hyperparameter controlling the sparsity penalty.",
            "Benchmark Tasks: Run Sparse-NEAT and standard NEAT on XOR, Circles, Moons, and Spiral classification tasks. Use standard datasets and split into train/test sets.",
            "Metrics: Record test accuracy (or MSE), number of connections in the evolved network (sparsity), number of fitness evaluations to convergence (e.g., when accuracy > 95% or fitness plateaus), and generalization error on held-out test data.",
            "Hyperparameter Tuning: Perform a grid search over \u03bb values (e.g., 0.001, 0.01, 0.1) to find the optimal balance between performance and sparsity for each task.",
            "Comparison: Compare results with standard NEAT and analyze trade-offs using metrics above. Additionally, compute computational efficiency metrics like average inference time or FLOPs for evolved networks.",
            "Ablation Study: Test variants where sparsity is normalized (e.g., by input size) to see if it improves consistency across tasks."
        ],
        "Risk Factors and Limitations": [
            "Sensitivity to the \u03bb hyperparameter; improper choice could lead to poor performance or insufficient sparsity, requiring careful tuning.",
            "Potential performance degradation if sparsity is over-emphasized, especially in tasks requiring dense connectivity.",
            "Increased computational overhead per evaluation due to sparsity calculation, though it should be minimal compared to fitness evaluation.",
            "May not generalize well to complex, high-dimensional tasks beyond the provided benchmarks.",
            "Evolution might get stuck in local optima where sparse networks underperform, necessitating mechanisms like adaptive \u03bb or novelty search."
        ]
    },
    {
        "Name": "evolving_plasticity_neat",
        "Title": "Evolving-Plasticity-NEAT: Co-evolving Neural Topologies and Synaptic Plasticity Rules for Enhanced Adaptability",
        "Short Hypothesis": "Co-evolving synaptic plasticity rules alongside neural network topologies in NEAT will enable networks to dynamically adapt during their lifetime, leading to improved performance, generalization, and adaptability to novel tasks, as this allows evolution to discover optimal plasticity mechanisms tailored to each topology, which is not possible with fixed or hand-designed plasticity rules and provides a more biologically plausible and efficient approach.",
        "Related Work": "NEAT and its variants focus on evolving topologies with fixed or pre-defined plasticity rules (e.g., SO-NEAT incorporates plasticity but does not evolve the rules themselves). Work on plasticity in neural networks, such as STDP and homeostatic plasticity (e.g., Effenberger et al., 2015), uses fixed rules, and while some evolutionary algorithms evolve parameters, they do not co-evolve plasticity rules with topologies in a neuroevolution context. This proposal is novel because it extends NEAT's genome to include evolvable plasticity rule parameters, allowing simultaneous optimization of structure and adaptation mechanisms\u2014a non-trivial integration that leverages evolution to discover effective plasticity, distinct from prior work.",
        "Abstract": "We propose Evolving-Plasticity-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that co-evolves both neural network topologies and synaptic plasticity rules. By encoding plasticity rule parameters (e.g., learning rates, decay factors) in the genome and subjecting them to mutation and crossover, networks can adapt dynamically during their lifetime based on evolved rules. We hypothesize that this approach will yield networks with superior performance, generalization, and adaptability to task changes, as evolution can tailor plasticity to specific topologies. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will compare against standard NEAT and fixed-plasticity variants, measuring accuracy, generalization error, and adaptability metrics. This work introduces a new paradigm for evolving adaptive AI systems.",
        "Experiments": [
            "Benchmark Tasks: Implement Evolving-Plasticity-NEAT on XOR, Circles, Moons, and Spiral classification tasks. Extend NEAT's genome to include plasticity parameters (e.g., for a simple Hebbian-like rule: learning rate \u03b7 and decay factor \u03bb, with initial random values).",
            "Plasticity Rule Implementation: During each fitness evaluation, allow networks to undergo within-lifetime plasticity updates based on their evolved rules (e.g., weight changes \u0394w = \u03b7 * pre * post - \u03bb * w for each connection over a fixed number of steps on training data).",
            "Comparison Baselines: Run standard NEAT (no plasticity), NEAT with fixed plasticity (e.g., \u03b7=0.1, \u03bb=0.01), and Evolving-Plasticity-NEAT. Metrics: number of evaluations to convergence (95% accuracy), final test accuracy, generalization error on held-out data, and performance on task variants (e.g., shifted distributions) to assess adaptability.",
            "Analysis of Evolved Rules: Examine the distribution of evolved plasticity parameters (\u03b7, \u03bb) and correlate with network performance and complexity to understand what rules are beneficial.",
            "Ablation Study: Test the importance of co-evolution by fixing plasticity parameters to evolved averages and comparing performance."
        ],
        "Risk Factors and Limitations": [
            "Increased computational cost per evaluation due to within-lifetime plasticity updates, which could slow down evolution.",
            "Sensitivity to the choice of plasticity rule parameterization; overly complex rules might not evolve effectively or could lead to instability.",
            "Potential for overfitting if plasticity allows networks to memorize training data too easily, reducing generalization.",
            "Hyperparameter tuning for mutation rates and bounds on plasticity parameters adds complexity.",
            "May not scale well to high-dimensional tasks without careful design of the plasticity rule space."
        ]
    },
    {
        "Name": "env_feedback_neat",
        "Title": "Env-Feedback-NEAT: Enhancing NeuroEvolution with Real-Time Environmental Feedback for Self-Organization",
        "Short Hypothesis": "Integrating real-time environmental feedback to dynamically adjust self-organization parameters (e.g., plasticity rates, homeostatic targets) during a network's lifetime in NEAT will lead to more adaptable and efficient neural networks that outperform static evolution in dynamic environments, as this allows continuous, context-sensitive adaptation that is not possible with fixed or pre-evolved parameters alone, and it is the best setting because it directly leverages environmental cues for fine-tuning adaptation mechanisms during evolution.",
        "Related Work": "NEAT and its extensions like SO-NEAT incorporate self-organization but with static or evolved parameters that do not change during a network's lifetime based on environmental input. Work in adaptive systems and reinforcement learning uses environmental feedback (e.g., in reward signals), but it is not integrated into neuroevolution's self-organization processes. This proposal is novel because it introduces a feedback loop where environmental signals directly modulate self-organization parameters in real-time, distinguishing it from curriculum learning (which sequences tasks) or robustness approaches (which handle perturbations statically), and it is not a trivial extension of existing NEAT variants.",
        "Abstract": "Env-Feedback-NEAT extends the NeuroEvolution of Augmenting Topologies algorithm by incorporating real-time environmental feedback to dynamically adjust self-organization parameters, such as synaptic plasticity rates and homeostatic regulation targets, during a network's lifetime. This enables networks to adapt more responsively to changing conditions. We hypothesize that this approach will improve performance, adaptability, and generalization in dynamic environments compared to standard NEAT and SO-NEAT. Experiments on dynamic variants of benchmarks like XOR, Circles, Moons, and Spiral\u2014where environmental feedback signals are derived from task dynamics\u2014will validate these claims by measuring convergence speed, adaptability metrics, and generalization error.",
        "Experiments": [
            "Dynamic Benchmark Tasks: Modify standard benchmarks to include environmental changes. For XOR, introduce time-varying noise or pattern shifts; for Circles and Moons, slowly move class centers or change radii over time. Generate a feedback signal (e.g., a scalar between 0 and 1) based on environmental change rate or error, which modulates self-organization parameters like plasticity rate (e.g., new_rate = base_rate * feedback).",
            "Implementation: Extend NEAT to include feedback channels. During each fitness evaluation, networks receive the feedback signal and adjust their self-organization parameters accordingly (e.g., Hebbian plasticity rate is scaled by feedback). Use a simple feedback function, such as feedback = 1 - exp(-|change_rate|) to increase adaptation in dynamic phases.",
            "Comparison: Run Env-Feedback-NEAT against standard NEAT and SO-NEAT (with fixed self-organization) on dynamic tasks. Metrics: number of evaluations to convergence (e.g., maintaining >90% accuracy despite changes), adaptability score (average performance during and after changes), generalization error on unseen dynamic variations, and network complexity.",
            "Ablation Study: Variate the feedback mechanism (e.g., different feedback functions or parameters) to analyze its impact and find optimal settings.",
            "Control Experiments: Test with static environments to ensure feedback does not harm performance when not needed."
        ],
        "Risk Factors and Limitations": [
            "Designing meaningful and generalizable environmental feedback signals may require domain knowledge or heuristics, limiting applicability to new tasks.",
            "Increased computational overhead per evaluation due to real-time feedback processing and parameter adjustments, though it should be minimal compared to fitness evaluation.",
            "Potential instability if feedback amplifies noise or leads to oscillatory behavior in self-organization, requiring careful tuning of feedback sensitivity.",
            "May not outperform in completely static environments, where feedback could introduce unnecessary complexity.",
            "Hyperparameter sensitivity for feedback functions and self-organization parameters adds tuning complexity compared to baseline NEAT."
        ]
    },
    {
        "Name": "temporal_coherence_neat",
        "Title": "Temporal-Coherence-NEAT: Enhancing NeuroEvolution with Mutation Sequence Optimization",
        "Short Hypothesis": "Incorporating temporal coherence into mutation sequences in NEAT\u2014by biasing mutations based on recent evolutionary changes\u2014will reduce inefficient explorations and accelerate convergence, as it promotes structured, build-upon-prior mutations that are more likely to be beneficial. This is the best setting because it directly addresses the randomness in NEAT's mutation operator without requiring complex external models or domain knowledge, and simpler alternatives like fixed mutation rates or random walks do not capture the temporal dependencies in evolutionary progress.",
        "Related Work": "NEAT relies on random mutations for topology evolution, with extensions focusing on speciation or plasticity but not on the sequence of mutations. Work in evolutionary algorithms sometimes uses mutation strategies (e.g., in evolution strategies), but they are not applied to neuroevolution's topological changes or based on temporal coherence. This proposal is novel because it introduces a lightweight, memory-based approach to guide mutations in NEAT by learning from recent mutation histories, distinguishing it from prior work that either randomizes or uses static mutation policies.",
        "Abstract": "We propose Temporal-Coherence-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that optimizes mutation sequences using temporal coherence. By maintaining a short-term memory of recent mutations and favoring sequences that have led to fitness improvements, we bias future mutations towards coherent, productive changes. We hypothesize that this will reduce the number of evaluations needed for convergence and improve solution quality by minimizing wasteful mutations. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will compare against standard NEAT, measuring evaluations to convergence, fitness, and sequence coherence metrics.",
        "Experiments": [
            "Implementation: Extend NEAT to include a coherence mechanism. For each generation, store the last k mutations (e.g., k=5) and their fitness impacts. Use a simple probability model: if a mutation sequence in memory is associated with fitness increase, increase the probability of similar mutations (e.g., same type: add node, add connection) in the next step.",
            "Benchmark Tasks: Run Temporal-Coherence-NEAT and standard NEAT on XOR, Circles, Moons, and Spiral classification tasks. Use standard datasets with train/test splits.",
            "Metrics: Record the number of fitness evaluations to reach convergence (e.g., 95% accuracy), final test accuracy or MSE, and a coherence score (e.g., proportion of mutations that are extensions of beneficial sequences).",
            "Ablation Study: Variate the memory length k and probability weighting to analyze sensitivity and optimal settings.",
            "Generalization Test: Evaluate on held-out test data to ensure coherence does not harm generalization."
        ],
        "Risk Factors and Limitations": [
            "Increased memory and computation per generation for tracking mutation sequences, though minimal for small k.",
            "Sensitivity to the choice of k and probability model; poor settings could lead to premature convergence or reduced exploration.",
            "May not perform well in tasks where beneficial mutations are highly irregular or non-sequential.",
            "Implementation complexity could introduce bugs compared to standard NEAT.",
            "Potential overfitting to short-term trends if coherence is too strong, neglecting long-term exploration."
        ]
    },
    {
        "Name": "meta_evolution_neat",
        "Title": "Meta-Evolution-NEAT: Self-Adaptive NeuroEvolution of Augmenting Topologies via Meta-Evolution of Hyperparameters",
        "Short Hypothesis": "Evolving NEAT's hyperparameters (e.g., mutation rates) through a meta-evolutionary process will lead to more efficient and robust neuroevolution by automatically adapting to the task at hand, reducing the need for manual tuning and improving convergence speed and solution quality. This is the best setting because it leverages evolution to optimize its own parameters in a integrated manner, which is not achieved through static hyperparameters or separate tuning methods like grid search, and it directly addresses the sensitivity of NEAT to hyperparameter settings.",
        "Related Work": "NEAT relies on fixed hyperparameters that are typically set manually or tuned via external methods like grid search. While self-adaptation is used in some evolutionary algorithms (e.g., in evolution strategies for parameter optimization), it has not been applied to NEAT's topological evolution or hyperparameters. Recent work on hyperparameter optimization in neuroevolution often treats it as a separate process (e.g., using Bayesian optimization), not integrated into the evolutionary framework. This proposal is novel because it introduces a meta-evolution approach specifically for NEAT, where hyperparameters are co-evolved with topologies, distinguishing it from prior work by embedding adaptation directly into the evolutionary process without external tuning.",
        "Abstract": "We propose Meta-Evolution-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that uses a meta-evolutionary process to self-adapt its hyperparameters, such as mutation and crossover rates. A higher-level evolutionary algorithm optimizes these hyperparameters for a lower-level NEAT run, enabling automatic adaptation to different tasks. We hypothesize that this approach will reduce the number of evaluations needed for convergence, improve robustness across tasks, and eliminate manual hyperparameter tuning. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will compare Meta-Evolution-NEAT against standard NEAT with fixed hyperparameters, measuring convergence speed, final fitness, generalization error, and evolved hyperparameter dynamics.",
        "Experiments": [
            "Implement Meta-Evolution-NEAT: Use a simple genetic algorithm for the meta-level to evolve hyperparameters (e.g., node addition rate, connection addition rate, weight mutation rate) over generations. The fitness for the meta-level is the performance of the lower-level NEAT run on a task.",
            "Benchmark Tasks: Run on XOR, Circles, Moons, and Spiral classification tasks. Compare against standard NEAT with best fixed hyperparameters (from literature or grid search) and a baseline with random hyperparameters.",
            "Metrics: Record number of evaluations to convergence (e.g., 95% accuracy on training data), final test accuracy or mean squared error (MSE), generalization error on held-out test data, and the values of evolved hyperparameters over generations.",
            "Ablation Study: Test the importance of meta-evolution by fixing hyperparameters to the average evolved values from Meta-Evolution-NEAT and comparing performance to see if adaptation is key.",
            "Computational Analysis: Measure wall-clock time and number of evaluations to assess overhead, ensuring it is feasible for academic labs."
        ],
        "Risk Factors and Limitations": [
            "Increased computational cost due to the meta-evolution process, as each meta-evaluation requires a full NEAT run, potentially making it slower than standard NEAT.",
            "Sensitivity to the meta-level algorithm's own hyperparameters (e.g., population size, mutation rates), which might need tuning, adding complexity.",
            "Risk of meta-evolution converging to suboptimal hyperparameters if the meta-fitness landscape is noisy or deceptive.",
            "May not scale well to very complex tasks due to computational constraints, limiting applicability beyond simple benchmarks.",
            "Implementation complexity in synchronizing two evolutionary levels could introduce bugs or inefficiencies."
        ]
    },
    {
        "Name": "transfer_init_neat",
        "Title": "Transfer-Init-NEAT: Accelerating NeuroEvolution with Transfer-Learning-Based Population Initialization",
        "Short Hypothesis": "Initializing NEAT populations with individuals pre-evolved on related or simpler tasks will significantly reduce the number of evaluations needed for convergence and improve generalization by biasing the search towards promising topological regions, as this leverages transfer learning to provide a head start that random initialization cannot offer. This is the best setting because it directly addresses the inefficiency of starting from scratch in NEAT, and simpler methods like parameter tuning or fixed seeds do not systematically incorporate cross-task knowledge.",
        "Related Work": "NEAT typically initializes populations with minimal, random topologies. While some work uses seeding or prior knowledge in evolutionary algorithms (e.g., ANEAT for coal and gas prediction uses feature-based initialization), no prior research systematically applies transfer learning across tasks to initialize NEAT populations. This proposal is novel because it introduces a general framework for using pre-evolved networks from source tasks to bias initial populations in target tasks, distinguishing it from task-specific seeding or meta-learning approaches that require complex architectures.",
        "Abstract": "We propose Transfer-Init-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that uses transfer learning to initialize populations with individuals pre-evolved on related tasks. By starting evolution with networks that have already adapted to similar problems, we hypothesize that Transfer-Init-NEAT will reduce the number of fitness evaluations required for convergence and improve generalization on target tasks. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will involve pre-evolving populations on simpler variants (e.g., 2-input XOR before 3-input, smaller circles before full) and comparing against standard NEAT with random initialization. Metrics will include evaluations to convergence, final accuracy, and generalization error, demonstrating the efficiency gains of biased initialization through transfer learning.",
        "Experiments": [
            "Pre-evolution on Source Tasks: For each benchmark (e.g., XOR), evolve a population on a simpler variant (e.g., 2-input XOR) using standard NEAT until convergence. Save the best individuals or a diverse subset.",
            "Initialization for Target Tasks: Initialize the population for the target task (e.g., standard XOR) with the pre-evolved individuals from the source task, supplemented with random individuals if needed to maintain population size.",
            "Comparison with Baselines: Run Transfer-Init-NEAT and standard NEAT (random initialization) on the target tasks. Metrics: number of fitness evaluations to achieve 95% accuracy, final test accuracy or MSE, generalization error on held-out data, and network complexity (nodes, connections).",
            "Transfer Scenarios: Test different source-target pairs: XOR 2-input to 3-input, small circles to large circles, simple moons to complex moons. Also, test negative transfer by using unrelated source tasks to ensure robustness.",
            "Ablation Study: Variate the proportion of pre-evolved individuals in the initial population (e.g., 25%, 50%, 100%) to analyze the impact on performance and convergence."
        ],
        "Risk Factors and Limitations": [
            "Risk of negative transfer if source and target tasks are unrelated, leading to worse performance than random initialization.",
            "Computational overhead from pre-evolving on source tasks, though this is a one-time cost that may be offset by faster convergence on target tasks.",
            "Potential for reduced diversity in the initial population if pre-evolved individuals are too similar, which could limit exploration and lead to local optima.",
            "Sensitivity to the choice of source tasks; requires careful selection to ensure relevance, which may need domain knowledge.",
            "May not scale well to very complex or high-dimensional tasks where transfer is less effective."
        ]
    },
    {
        "Name": "cooperative_neat",
        "Title": "Cooperative-NEAT: Evolving Emergent Cooperation in Multi-Agent Systems with NeuroEvolution of Augmenting Topologies",
        "Short Hypothesis": "Evolving neural network topologies for multiple agents simultaneously in NEAT, using shared or coordinated fitness functions, will enable the emergence of complex cooperative behaviors that outperform independent evolution or hand-designed coordination, as NEAT's topology evolution allows for adaptive and specialized agent architectures that are not possible with fixed-topology approaches or separate optimization.",
        "Related Work": "Existing work on multi-agent systems often uses reinforcement learning (e.g., MARL), evolutionary game theory, or standard evolutionary algorithms with fixed topologies (e.g., genetic algorithms for control). Papers like Wang et al. (2024) on multi-agent learning systems and Devi et al. (2024) on swarm robotics focus on cooperation but do not incorporate topology evolution. NEAT has been extended for single agents, but not systematically for multi-agent cooperation. This proposal is novel because it applies NEAT's speciation and incremental topology growth to multi-agent settings, enabling the evolution of diverse and complementary agent architectures that foster cooperation, which is not a trivial extension of prior NEAT variants or multi-agent methods.",
        "Abstract": "We propose Cooperative-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm for multi-agent systems, where neural networks for multiple agents are evolved simultaneously with fitness functions that reward collective performance. By leveraging NEAT's ability to evolve topologies and speciate, Cooperative-NEAT encourages the emergence of specialized and cooperative behaviors. We hypothesize that this approach will achieve higher levels of cooperation, better task performance, and improved scalability compared to independent evolution or fixed-topology methods. Experiments on benchmarks like Predator-Prey, Resource Collection, and Cooperative Navigation will measure cooperation metrics, fitness convergence, and generalization to novel scenarios.",
        "Experiments": [
            "Predator-Prey Task: Implement a grid-world where predators (agents) must catch prey. Use a shared fitness function based on team success (e.g., average capture time). Compare Cooperative-NEAT (evolve all agents together with shared fitness) against independent NEAT (each agent evolved separately) and a hand-designed baseline. Metrics: capture rate, number of evaluations to convergence, and a cooperation score (e.g., variance in agent contributions).",
            "Resource Collection Task: Agents must collect resources in a shared environment. Fitness is defined as total resources collected minus conflict penalties. Implement coordinated mutation where mutations in one agent can influence others based on compatibility distance. Metrics: resource collection efficiency, conflict rate, and network complexity (nodes/connections).",
            "Cooperative Navigation: Agents must navigate to targets without collisions. Use a multi-objective fitness function balancing individual and team rewards. After evolution, test on unseen target configurations to assess generalization. Metrics: success rate, collision avoidance, and adaptability score.",
            "Ablation Study: Variate the fitness function (e.g., fully shared vs. partially shared rewards) and speciation parameters to analyze their impact on cooperation and performance."
        ],
        "Risk Factors and Limitations": [
            "Increased computational cost due to evaluating multiple agents per fitness evaluation, which may limit scalability to large populations or complex environments.",
            "Difficulty in designing effective shared fitness functions that properly incentivize cooperation without stifling individual initiative, requiring careful tuning.",
            "Potential for premature convergence or loss of diversity if speciation is not well-managed in the multi-agent context.",
            "May not generalize well to highly heterogeneous tasks where agent roles are vastly different, limiting applicability.",
            "Implementation complexity in handling inter-agent dependencies during evolution, which could introduce bugs or inefficiencies."
        ]
    },
    {
        "Name": "rnn_neat",
        "Title": "RNN-NEAT: Evolving Recurrent Neural Network Topologies with NEAT for Long-Term Dependencies",
        "Short Hypothesis": "Extending NEAT to evolve recurrent neural network (RNN) topologies, including potential skip-connections and recurrent loops, will enable the discovery of architectures that handle long-term dependencies more effectively than standard RNNs or fixed-topology approaches, as evolution can tailor connectivity patterns to specific temporal tasks without relying on hand-designed gating mechanisms like LSTMs. This is the best setting because it leverages NEAT's strength in topological exploration to address a core limitation of RNNs, and simpler methods like gradient-based training or fixed architectures do not offer the same adaptive structural flexibility.",
        "Related Work": "NEAT is primarily used for feedforward networks, with limited application to RNNs. Work on RNNs for long-term dependencies often uses gated architectures like LSTMs or GRUs (Hochreiter & Schmidhuber, 1997), or training tricks like skip-connections (e.g., Soo et al., 2023 on biologically plausible RNNs). However, no prior research fully integrates NEAT's evolutionary topology optimization with RNNs for long-term dependency tasks. This proposal distinguishes itself by using NEAT to evolve recurrent and skip-connection structures dynamically, which is not a trivial extension and moves beyond fixed architectures or post-hoc modifications.",
        "Abstract": "We propose RNN-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm to evolve recurrent neural network topologies for handling long-term dependencies. By modifying NEAT's genome to include recurrent connections and enabling mutations that add recurrent links or skip-connections, RNN-NEAT can discover architectures tailored to temporal tasks without predefined gating. We hypothesize that this approach will outperform standard RNNs and LSTMs in tasks requiring long-term memory, with improved accuracy and faster convergence. Experiments on benchmarks like the adding problem, sequential MNIST, and time series prediction will validate performance against baselines, measuring accuracy, sequence length handling, and evolutionary efficiency.",
        "Experiments": [
            "Adding Problem: Implement RNN-NEAT and compare against standard RNN, LSTM, and NEAT (feedforward) on the adding problem with varying sequence lengths (e.g., 100, 500 steps). Metrics: mean squared error (MSE), number of evaluations to convergence (MSE < 0.01), and generalization to longer sequences.",
            "Sequential MNIST: Train on MNIST classification where pixels are fed sequentially. Compare accuracy and convergence speed of RNN-NEAT against LSTM and standard RNN. Use a population size of 100 and measure test accuracy after evolution.",
            "Time Series Prediction: Use a synthetic or real-world time series dataset (e.g., sunspots) with long-term dependencies. Evaluate forecasting performance with MSE and assess evolved topology characteristics (e.g., recurrence depth, skip-connections).",
            "Ablation Study: Variate the mutation rates for adding recurrent connections in RNN-NEAT to analyze impact on performance and topology evolution."
        ],
        "Risk Factors and Limitations": [
            "Increased computational cost due to evaluating RNNs on sequential data, which is slower than feedforward networks.",
            "Potential instability in evolved RNNs, such as vanishing gradients or chaotic behavior, requiring careful fitness evaluation and possible gradient clipping.",
            "Sensitivity to hyperparameters for recurrent mutation rates and compatibility distance calculations in NEAT.",
            "May not scale well to very high-dimensional or complex temporal tasks beyond the benchmarks used.",
            "Comparison with LSTMs might show limited gains if evolved topologies cannot match gated mechanisms efficiently."
        ]
    },
    {
        "Name": "continual_neat",
        "Title": "Continual-NEAT: NeuroEvolution for Lifelong Learning Without Catastrophic Forgetting",
        "Short Hypothesis": "Integrating memory replay and lightweight regularization into NEAT will enable evolved neural networks to learn sequential tasks without catastrophic forgetting, as NEAT's topology evolution can adaptively add specialized structures for new tasks while preserving old knowledge through replay, a approach not achievable with fixed architectures or standard NEAT alone. This is the best setting because it leverages NEAT's inherent ability to grow and speciate, providing a natural mechanism for task-specific compartments without complex external constraints.",
        "Related Work": "Catastrophic forgetting is addressed in continual learning via methods like replay (e.g., Lopez-Paz & Ranzato, 2017), regularization (e.g., EWC), and architectural changes (e.g., Rusu et al., 2016), but these are typically applied to fixed or hand-designed networks. NEAT has been used for single-task evolution but not for continual learning. Recent work like 'Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning' (Elsayed et al., 2024) focuses on gradient-based methods, not evolutionary topology search. This proposal is novel because it adapts NEAT for continual learning by incorporating replay and regularization directly into the evolutionary process, which is not a trivial extension and offers a new way to handle non-stationary tasks through structural evolution.",
        "Abstract": "We propose Continual-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm for continual learning, where networks evolve across sequential tasks without catastrophic forgetting. By integrating a memory replay buffer that stores samples from previous tasks and a lightweight regularization term penalizing changes to important weights, Continual-NEAT enables adaptive topology growth for new tasks while preserving performance on old ones. We hypothesize that this approach will maintain high accuracy across tasks with fewer evaluations compared to standard NEAT and replay-based baselines. Experiments on variants of XOR, Permuted MNIST, and a simple reinforcement learning task will measure forgetting rate, accuracy, and evolutionary efficiency, demonstrating the potential of evolutionary topology search for lifelong learning.",
        "Experiments": [
            "Sequential XOR Tasks: Train on XOR with 2 inputs, then 3 inputs, and finally standard XOR. Use a replay buffer storing 10% of data from each previous task. Fitness function includes task accuracy and a regularization term (e.g., L2 penalty on weight changes for connections crucial to old tasks, identified by average magnitude). Metrics: accuracy on all tasks after evolution, number of evaluations to achieve >90% accuracy on each new task, and forgetting rate (drop in accuracy on old tasks).",
            "Permuted MNIST: Implement a continual learning version where the pixel order is permuted for each new task. Compare Continual-NEAT against standard NEAT with replay and a baseline without replay. Use a population size of 100, and measure test accuracy across tasks after training on the sequence. Replay buffer size: 100 samples per task.",
            "Simple Reinforcement Learning Task: Use a grid-world navigation task with changing goals (e.g., first goal A, then goal B). Fitness is reward, and replay stores state-action pairs. Metrics: average reward on all goals, evaluations to convergence, and catastrophic forgetting score (difference in reward before and after new task learning).",
            "Ablation Study: Variate the replay buffer size (e.g., 5%, 10%, 20% of data) and regularization strength to analyze their impact on performance and forgetting."
        ],
        "Risk Factors and Limitations": [
            "Increased computational cost due to replay and regularization, which may slow down evolution, especially with large buffers.",
            "Sensitivity to hyperparameters like replay buffer size and regularization weight, requiring tuning for each task.",
            "Potential overfitting if replay is too dominant, reducing exploration for new tasks.",
            "May not scale well to complex, high-dimensional tasks beyond simple benchmarks due to NEAT's inherent limitations.",
            "Implementation complexity in managing the replay buffer and regularization within NEAT's framework could introduce bugs."
        ]
    },
    {
        "Name": "intrinsic_motivation_neat",
        "Title": "Intrinsic-Motivation-NEAT: Enhancing NeuroEvolution with Intrinsic Motivation for Exploration",
        "Short Hypothesis": "Incorporating intrinsic motivation (e.g., novelty or curiosity) directly into NEAT's fitness function will significantly improve exploration and performance in sparse-reward environments by encouraging diverse behaviors beyond task-specific rewards, leading to faster convergence and better generalization. This is the best setting because intrinsic motivation provides a continuous, internal drive for exploration that complements NEAT's topological evolution, unlike external reward shaping or random mutations, and it addresses a key limitation of NEAT in environments with delayed or sparse feedback.",
        "Related Work": "Intrinsic motivation is well-studied in reinforcement learning (e.g., curiosity-driven learning in RL by Pathak et al., 2017) and has been applied to evolutionary algorithms in limited contexts (e.g., novelty search in GA), but it has not been integrated into NEAT's evolutionary process. NEAT itself focuses on task fitness without intrinsic drives. This proposal is novel because it embeds intrinsic motivation directly into NEAT's fitness evaluation, allowing evolution to prioritize exploratory behaviors dynamically, which is not a trivial extension and differs from prior work that uses intrinsic motivation separately or in fixed architectures.",
        "Abstract": "We propose Intrinsic-Motivation-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that incorporates intrinsic motivation into the fitness function to enhance exploration in sparse-reward environments. By adding a novelty or curiosity term (e.g., based on behavioral diversity or prediction error) to the fitness score, networks are rewarded for discovering new states or reducing uncertainty, alongside task performance. We hypothesize that this approach will reduce the number of evaluations needed for convergence, improve performance in tasks with sparse rewards, and increase behavioral diversity. Experiments on maze navigation and simple reinforcement learning benchmarks will validate these claims against standard NEAT, measuring convergence speed, final reward, and exploration metrics.",
        "Experiments": [
            "Maze Navigation Task: Implement a grid-world maze with sparse rewards (reward only at goal). Use Intrinsic-Motivation-NEAT with a novelty-based fitness term (e.g., fitness = task_reward + \u03b2 * novelty_score, where novelty_score is the inverse frequency of visited states in a archive). Compare against standard NEAT. Metrics: number of evaluations to reach the goal, success rate, and coverage of maze states.",
            "Sparse-Reward RL Benchmark: Use a simple environment like Mountain Car or CartPole with sparse rewards. Incorporate a curiosity term based on prediction error (e.g., using a separate network to predict next states and adding error to fitness). Metrics: average reward per episode, convergence evaluations, and diversity of behaviors (e.g., state visitation entropy).",
            "Generalization Test: After evolution, test networks on unseen maze layouts or environment variations to assess generalization. Metrics: success rate on new environments and adaptability score.",
            "Ablation Study: Variate the weight \u03b2 of the intrinsic motivation term to analyze its impact on exploration and performance, and compare to a baseline with only task reward."
        ],
        "Risk Factors and Limitations": [
            "Increased computational overhead per evaluation due to calculating intrinsic motivation (e.g., maintaining novelty archive or prediction models), which could slow down evolution.",
            "Sensitivity to hyperparameters like the weight \u03b2, requiring careful tuning to balance exploration and exploitation.",
            "Potential for over-exploration if intrinsic motivation is too strong, leading to poor task performance or failure to converge.",
            "May not scale well to high-dimensional state spaces due to the curse of dimensionality in novelty or curiosity calculations.",
            "Implementation complexity in integrating intrinsic motivation seamlessly with NEAT's speciation and crossover mechanisms."
        ]
    },
    {
        "Name": "env_co_evolve_neat",
        "Title": "Env-Co-Evolve-NEAT: Co-Evolving Environments and Neural Topologies for Enhanced Robustness and Generalization",
        "Short Hypothesis": "Co-evolving environment parameters alongside neural network topologies in NEAT will expose agents to a diverse and adaptive set of challenges, leading to more robust and generalizable solutions without requiring complex external environment generators, as this dynamic co-evolution fosters continuous adaptation that static environments or separate optimization cannot achieve.",
        "Related Work": "Existing work like POET and Enhanced-POET co-evolves environments and agents using complex representations (e.g., CPPNs or LLMs), which are resource-intensive and not integrated with NEAT's evolutionary process. This proposal distinguishes itself by embedding simple environment mutation and selection directly into NEAT, using lightweight parameterized environments (e.g., maze layouts or task parameters) that are evolved based on agent performance. It is not a trivial extension, as it modifies NEAT's core to include environment evolution, unlike prior work that relies on separate, complex generators.",
        "Abstract": "We propose Env-Co-Evolve-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that co-evolves environment parameters alongside neural network topologies. Environments are represented as simple, mutable parameters (e.g., maze wall positions or classification task distortions), and are selected and mutated based on agent fitness, creating a dynamic co-evolutionary process. We hypothesize that this approach will produce agents with improved robustness, generalization, and adaptability to novel conditions by continuously challenging them with evolving environments. Experiments on benchmarks like evolving mazes and dynamic variants of Circles and Moons tasks will compare against standard NEAT with static environments, measuring convergence speed, generalization error, and environment diversity.",
        "Experiments": [
            "Evolving Maze Navigation: Implement a grid-world maze where the layout (wall positions and goal location) is evolved alongside agent topologies. Mutate environment parameters with simple operations (e.g., add/remove walls, shift goals) and select environments based on agent fitness (e.g., higher fitness environments are kept or mutated). Compare Env-Co-Evolve-NEAT against NEAT with fixed mazes. Metrics: number of evaluations to convergence (success rate >90%), generalization to unseen mazes, and diversity of evolved environments (number of unique layouts).",
            "Dynamic Classification Tasks: Use Circles and Moons datasets with evolving parameters (e.g., center positions, radii, or noise levels). Co-evolve these parameters with neural networks. Metrics: test accuracy on held-out data, adaptability to sudden parameter changes, and comparisons with standard NEAT on static tasks.",
            "Ablation Study: Variate the mutation rate for environments and the selection pressure (e.g., how often environments are updated) to analyze impact on performance and diversity."
        ],
        "Risk Factors and Limitations": [
            "Increased computational cost per evaluation due to environment mutations and fitness assessments, though it should be manageable with simple environments.",
            "Sensitivity to co-evolution parameters (e.g., mutation rates), requiring careful tuning to avoid environments becoming too easy or too hard.",
            "Potential for reduced exploration if co-evolution focuses on narrow environment niches, limiting agent diversity.",
            "May not scale well to high-dimensional or complex environments beyond simple benchmarks.",
            "Implementation complexity in synchronizing environment and agent evolution could introduce bugs."
        ]
    },
    {
        "Name": "adaptive_mutation_neat",
        "Title": "Adaptive-Mutation-NEAT: Dynamic Mutation Rate Adjustment Based on Population Diversity",
        "Short Hypothesis": "Dynamically adjusting mutation rates in NEAT based on real-time population diversity metrics (e.g., number of species or average behavioral difference) will improve exploration-exploitation balance, leading to faster convergence and better solution quality without complex external mechanisms, as this provides a responsive way to maintain diversity that fixed rates or separate algorithms cannot achieve.",
        "Related Work": "NEAT uses fixed mutation rates, and while extensions like AGENT (Behjat et al., 2023) adapt genetic operators with graph-theoretic diversity, they introduce significant complexity. This proposal is novel because it integrates a simple, real-time diversity feedback mechanism directly into NEAT's mutation process using lightweight metrics like speciation count, which is not a trivial extension and differs from prior work by focusing solely on mutation rate adaptation without altering core speciation or selection.",
        "Abstract": "We propose Adaptive-Mutation-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that dynamically adjusts mutation rates for adding nodes and connections based on population diversity. Diversity is measured in real-time using the number of species or average behavioral differences between networks. When diversity drops below a threshold, mutation rates increase to encourage exploration; when diversity is high, rates decrease to favor exploitation. We hypothesize that this approach will reduce the number of evaluations needed for convergence and improve performance on complex tasks by maintaining optimal diversity. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will validate against standard NEAT, measuring convergence speed, accuracy, and diversity metrics.",
        "Experiments": [
            "Benchmark Tasks: Implement Adaptive-Mutation-NEAT and standard NEAT on XOR, Circles, Moons, and Spiral classification tasks. Use standard datasets with train/test splits.",
            "Diversity Measurement: Compute diversity in real-time using (1) the number of species in the population (from NEAT's speciation) and (2) average behavioral difference (e.g., output variance on a fixed input set). Set thresholds for low and high diversity (e.g., low if species count < 5, high if > 10).",
            "Mutation Rate Adaptation: Adjust mutation rates for adding nodes and connections dynamically each generation. For example: if diversity is low, multiply mutation rates by a factor (e.g., 1.5); if high, divide by a factor (e.g., 0.8). Use initial rates from standard NEAT.",
            "Metrics: Record the number of fitness evaluations to reach convergence (e.g., 95% accuracy on training data), final test accuracy or MSE, generalization error on held-out data, and diversity metrics (species count, behavioral variance) over generations.",
            "Comparison: Run against standard NEAT with fixed mutation rates. Perform statistical tests to confirm significance of improvements.",
            "Ablation Study: Variate the diversity thresholds and adaptation factors to analyze sensitivity and find optimal settings."
        ],
        "Risk Factors and Limitations": [
            "Sensitivity to the choice of diversity metric and thresholds; poor settings could lead to oscillatory behavior or no improvement.",
            "Increased computational overhead per generation due to diversity calculation, though it should be minimal for simple metrics.",
            "Potential for reduced performance if adaptation is too aggressive, causing instability in mutation rates.",
            "May not generalize well to tasks where diversity metrics do not correlate with fitness improvement.",
            "Implementation requires careful integration with NEAT's existing speciation mechanism to avoid conflicts."
        ]
    },
    {
        "Name": "noise_injected_neat",
        "Title": "Noise-Injected-NEAT: Enhancing NeuroEvolution with Stochastic Regularization",
        "Short Hypothesis": "Injecting controlled noise into NEAT's mutation operations and fitness evaluations will act as a stochastic regularizer during evolution, leading to improved generalization, robustness, and faster convergence by preventing overfitting and encouraging exploration, which is not achievable with deterministic mutations or post-hoc regularization methods. This is the best setting because it leverages noise's simplicity and effectiveness in a evolutionary context where topology growth is inherently noisy, and simpler alternatives like fixed mutation rates do not provide adaptive regularization.",
        "Related Work": "Noise injection is widely used in neural network training (e.g., dropout, weight noise) and some evolutionary algorithms for diversity maintenance, but it has not been systematically integrated into NEAT's mutation and fitness evaluation processes. Prior NEAT extensions focus on speciation, plasticity, or surrogate models, but none explicitly use noise for regularization during evolution. This proposal distinguishes itself by embedding noise directly into NEAT's core operations, offering a lightweight, non-trivial enhancement that complements existing methods without altering NEAT's speciation or historical markings.",
        "Abstract": "We propose Noise-Injected-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm that incorporates controlled noise injection into mutation operations (e.g., weight perturbations) and fitness evaluations to serve as a stochastic regularizer. By adding Gaussian noise to mutations and fitness scores, we aim to enhance exploration, prevent overfitting, and improve generalization. We hypothesize that this approach will reduce the number of evaluations needed for convergence, increase robustness to input variations, and yield networks with better performance on unseen data. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will compare against standard NEAT, measuring convergence speed, accuracy, generalization error, and noise sensitivity.",
        "Experiments": [
            "Implement Noise-Injected-NEAT with Gaussian noise added to: (1) weight mutations (e.g., noise ~ N(0, \u03c3_w) with \u03c3_w tuned via grid search over [0.01, 0.1, 0.5]), and (2) fitness evaluations (e.g., add noise ~ N(0, \u03c3_f) with \u03c3_f in [0.05, 0.2] to fitness scores during selection).",
            "Benchmark Tasks: Run on XOR, Circles, Moons, and Spiral classification tasks. Use standard datasets with train/test splits.",
            "Metrics: Record number of fitness evaluations to achieve 95% accuracy on training data, final test accuracy or mean squared error (MSE), generalization error on held-out test data, and robustness to input noise (e.g., test accuracy with noisy inputs).",
            "Comparison: Compare against standard NEAT without noise injection. Perform statistical tests to confirm significance.",
            "Ablation Study: Variate noise levels (\u03c3_w and \u03c3_f) to analyze impact on performance and find optimal settings. Also test with noise only in mutations or only in fitness to isolate effects."
        ],
        "Risk Factors and Limitations": [
            "Sensitivity to noise levels; improper tuning could hinder performance or lead to instability, requiring careful hyperparameter search.",
            "Increased computational overhead per evaluation due to noise generation and application, though minimal compared to fitness evaluation.",
            "Potential for reduced convergence if noise is too high, causing excessive exploration or fitness distortion.",
            "May not generalize well to very complex or high-dimensional tasks beyond simple benchmarks.",
            "Implementation must ensure noise does not interfere with NEAT's speciation mechanism, potentially requiring adjustments to compatibility distance calculations."
        ]
    },
    {
        "Name": "evo_act_neat",
        "Title": "Evo-Act-NEAT: NeuroEvolution of Augmenting Topologies with Evolvable Activation Functions",
        "Short Hypothesis": "Evolving activation function types for each neuron in NEAT will enhance network expressivity and performance on non-linear tasks by allowing tailored non-linearities, which fixed activation functions cannot provide, and this is best investigated within NEAT's evolutionary framework as it naturally incorporates diversity through speciation and does not require complex external mechanisms.",
        "Related Work": "Standard NEAT uses fixed activation functions (typically sigmoid or tanh). While some neuroevolution methods like Cartesian Genetic Programming evolve activation functions, they lack NEAT's speciation and historical markings. Recent NAS approaches focus on architecture search but not on evolving activation functions within an evolutionary context. This proposal is novel because it integrates evolvable activation functions into NEAT's genome, enabling a richer search space without altering core mechanisms, and it is not a trivial extension as it addresses a fundamental aspect of neural network design that is often overlooked in NEAT extensions.",
        "Abstract": "We propose Evo-Act-NEAT, an extension of the NeuroEvolution of Augmenting Topologies (NEAT) algorithm where the activation function of each neuron is encoded in the genome and subject to mutation and crossover. This allows evolution to discover optimal non-linearities for different parts of the network, potentially improving expressivity and task performance. We hypothesize that Evo-Act-NEAT will achieve higher accuracy and faster convergence on complex non-linear tasks compared to standard NEAT with fixed activation functions. Experiments on benchmarks like XOR, Circles, Moons, and Spiral will evaluate performance metrics and analyze the diversity of evolved activation functions, demonstrating the benefits of this approach.",
        "Experiments": [
            "Implement Evo-Act-NEAT by extending NEAT's genome to include an activation function gene per neuron, with possible functions: sigmoid, tanh, ReLU, leaky ReLU. Mutation operations can change this gene with a specified probability (e.g., 0.1).",
            "Benchmark Tasks: Conduct experiments on XOR, Circles, Moons, and Spiral classification tasks using standard datasets. Compare Evo-Act-NEAT against baseline NEAT with fixed sigmoid activation and a fixed ReLU variant.",
            "Metrics: Record the number of fitness evaluations required to reach convergence (e.g., 95% training accuracy), final test accuracy, generalization error on held-out data, and the distribution of activation functions in the best-evolved networks.",
            "Ablation Study: Variate the mutation rate for activation functions (e.g., 0.05, 0.1, 0.2) to assess sensitivity and optimal settings, and compare performance across different activation function palettes."
        ],
        "Risk Factors and Limitations": [
            "Increased search space may slow down evolution or lead to longer convergence times due to additional genetic diversity.",
            "Sensitivity to the choice of activation function set; including too many or inappropriate functions could hinder performance or introduce bias.",
            "Potential for overfitting if evolved activation functions become too specialized to training data, reducing generalization.",
            "May not show significant improvements on very simple tasks where fixed activation functions are sufficient, limiting perceived impact.",
            "Implementation complexity in modifying NEAT's genome and compatibility distance calculations to account for activation function differences."
        ]
    },
    {
        "Name": "dynamic_mo_neat",
        "Title": "Dynamic-MO-NEAT: Dynamic Multi-Objective NeuroEvolution of Augmenting Topologies",
        "Short Hypothesis": "Extending NEAT to handle dynamic multi-objective optimization by incorporating time-varying fitness functions and transfer learning via speciation will enable efficient adaptation to changing environments and objectives, outperforming static NEAT and other EAs in convergence speed and solution quality, as NEAT's topology evolution and speciation provide a natural mechanism for maintaining diversity and leveraging historical information.",
        "Related Work": "NEAT is designed for static single-objective optimization, while multi-objective EAs like NSGA-II and MOEA/D handle static multi-objective problems. Dynamic multi-objective optimization has been addressed with transfer learning and prediction strategies in general EAs (e.g., Zhang et al., 2023; Yao et al., 2023), but not with NEAT. This proposal is novel because it adapts NEAT's core mechanisms\u2014speciation, historical markings, and incremental growth\u2014to dynamic multi-objective settings, which is not a trivial extension and leverages NEAT's strengths in topology search for neural networks.",
        "Abstract": "We propose Dynamic-MO-NEAT, an extension of the NeuroEvolution of Augmenting Topologies algorithm for dynamic multi-objective optimization. By modifying NEAT to support time-varying fitness functions that incorporate multiple objectives (e.g., accuracy and sparsity) and integrating transfer learning through species-based knowledge retention, Dynamic-MO-NEAT can adapt to environmental changes without catastrophic forgetting. We hypothesize that this approach will achieve faster convergence, better hypervolume and inverted generational distance (IGD) metrics, and improved adaptability compared to static NEAT and other DMO algorithms. Experiments on dynamic variants of XOR, Circles, Moons, and Spiral tasks will validate these claims, with changes introduced by varying task parameters over time.",
        "Experiments": [
            "Dynamic XOR Task: Modify XOR to have objectives change over generations (e.g., minimize error and maximize sparsity). Implement Dynamic-MO-NEAT with a fitness function that weights objectives dynamically. Metrics: hypervolume, IGD, number of evaluations to convergence, and species count over time.",
            "Dynamic Classification Tasks: Use Circles and Moons datasets with time-varying parameters (e.g., changing class separability or noise levels). Compare against standard NEAT and a baseline multi-objective EA like NSGA-II. Metrics: accuracy, sparsity, generalization error, and adaptability score after changes.",
            "Transfer Learning Evaluation: After evolution on one dynamic task, test on a related but unseen dynamic task to assess transfer performance. Use species information to initialize populations. Metrics: fitness recovery time and performance on new task.",
            "Ablation Study: Variate the rate of environmental change and the weight adaptation strategy in the fitness function to analyze sensitivity."
        ],
        "Risk Factors and Limitations": [
            "Increased computational cost due to multi-objective fitness evaluation and transfer learning mechanisms, which may slow down evolution.",
            "Sensitivity to hyperparameters for objective weighting and change detection, requiring careful tuning.",
            "Potential for overfitting to specific change patterns if not generalized properly.",
            "May not scale well to high-dimensional or complex dynamic environments beyond simple benchmarks.",
            "Implementation complexity in modifying NEAT's core for dynamic objectives and transfer learning."
        ]
    }
]